{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uRC8olfZCPU"
      },
      "source": [
        "# Introduction to machine learning & Data Analysis\n",
        "\n",
        "Basic introduction on how to perform typical machine learning tasks with Python.\n",
        "\n",
        "Prepared by Mykhailo Vladymyrov & Aris Marcolongo,\n",
        "Data Science Lab, University Of Bern, 2022\n",
        "\n",
        "This work is licensed under <a href=\"https://creativecommons.org/share-your-work/public-domain/cc0/\">CC0</a>.\n",
        "\n",
        "# Part 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-fYVr8NTLeK"
      },
      "source": [
        "# Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVJn0ilgOS8F",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "from sklearn import ensemble\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from matplotlib import  pyplot as plt\n",
        "import seaborn as sns\n",
        "#sns.set()\n",
        "\n",
        "from time import time as timer\n",
        "from imageio import imread\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import tensorflow as tf\n",
        "import tarfile\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdMxnUooXJvr"
      },
      "outputs": [],
      "source": [
        "pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oes8tZkKXS1S"
      },
      "outputs": [],
      "source": [
        "import umap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xQIE54NmqJs"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('data'):\n",
        "    path = os.path.abspath('.')+'/colab_material.tgz'\n",
        "    tf.keras.utils.get_file(path, 'https://github.com/neworldemancer/DSF5/raw/master/colab_material.tgz')\n",
        "    tar = tarfile.open(path, \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atch-QcelhRy"
      },
      "outputs": [],
      "source": [
        "from utils.routines import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBxkWZaLCUcR"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MssLYOiCUcX"
      },
      "source": [
        "In this course we will use several synthetic and real-world datasets to illustrate the behavior of the models and exercise our skills."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UQgU5I-lEll"
      },
      "source": [
        "## 1. Synthetic linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGfWOWRjlWPa"
      },
      "outputs": [],
      "source": [
        "def get_linear(n_d=1, n_points=10, w=None, b=None, sigma=5):\n",
        "  x = np.random.uniform(0, 10, size=(n_points, n_d))\n",
        "  \n",
        "  w = w or np.random.uniform(0.1, 10, n_d)\n",
        "  b = b or np.random.uniform(-10, 10)\n",
        "  y = np.dot(x, w) + b + np.random.normal(0, sigma, size=n_points)\n",
        "\n",
        "  print('true slopes: w =', w, ';  b =', b)\n",
        "\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RLYxGy_nBZG"
      },
      "outputs": [],
      "source": [
        "x, y = get_linear(n_d=1, sigma=0)\n",
        "plt.plot(x[:, 0], y, '*')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10ODDOp4nX4S"
      },
      "outputs": [],
      "source": [
        "n_d = 2\n",
        "x, y = get_linear(n_d=n_d, n_points=100)\n",
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x[:,0], x[:,1], y, marker='x', color='b',s=40)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2uMw5C4CUcj"
      },
      "source": [
        "## 2. House prices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi_Q6YTNCUcj"
      },
      "source": [
        "Subset of the Ames Houses dataset: http://jse.amstat.org/v19n3/decock.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj2LFlahCUcj"
      },
      "outputs": [],
      "source": [
        "def house_prices_dataset(return_df=False, price_max=400000, area_max=40000):\n",
        "  path = 'data/AmesHousing.csv'\n",
        "\n",
        "  df = pd.read_csv(path, na_values=('NaN', ''), keep_default_na=False)\n",
        "  \n",
        "  rename_dict = {k:k.replace(' ', '').replace('/', '') for k in df.keys()}\n",
        "  df.rename(columns=rename_dict, inplace=True)\n",
        "  \n",
        "  useful_fields = ['LotArea',\n",
        "                  'Utilities', 'OverallQual', 'OverallCond',\n",
        "                  'YearBuilt', 'YearRemodAdd', 'ExterQual', 'ExterCond',\n",
        "                  'HeatingQC', 'CentralAir', 'Electrical',\n",
        "                  '1stFlrSF', '2ndFlrSF','GrLivArea',\n",
        "                  'FullBath', 'HalfBath',\n",
        "                  'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n",
        "                  'Functional','PoolArea',\n",
        "                  'YrSold', 'MoSold'\n",
        "                  ]\n",
        "  target_field = 'SalePrice'\n",
        "\n",
        "  df.dropna(axis=0, subset=useful_fields+[target_field], inplace=True)\n",
        "\n",
        "  cleanup_nums = {'Street':      {'Grvl': 0, 'Pave': 1},\n",
        "                  'LotFrontage': {'NA':0},\n",
        "                  'Alley':       {'NA':0, 'Grvl': 1, 'Pave': 2},\n",
        "                  'LotShape':    {'IR3':0, 'IR2': 1, 'IR1': 2, 'Reg':3},\n",
        "                  'Utilities':   {'ELO':0, 'NoSeWa': 1, 'NoSewr': 2, 'AllPub': 3},\n",
        "                  'LandSlope':   {'Sev':0, 'Mod': 1, 'Gtl': 3},\n",
        "                  'ExterQual':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'ExterCond':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'BsmtQual':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'BsmtCond':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'BsmtExposure':{'NA':0, 'No':1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
        "                  'BsmtFinType1':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
        "                  'BsmtFinType2':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
        "                  'HeatingQC':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'CentralAir':  {'N':0, 'Y': 1},\n",
        "                  'Electrical':  {'':0, 'NA':0, 'Mix':1, 'FuseP':2, 'FuseF': 3, 'FuseA': 4, 'SBrkr': 5},\n",
        "                  'KitchenQual': {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'Functional':  {'Sal':0, 'Sev':1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2':5, 'Min1':6, 'Typ':7},\n",
        "                  'FireplaceQu': {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'PoolQC':      {'NA':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'Fence':       {'NA':0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv':4},\n",
        "                  }\n",
        "\n",
        "  df_X = df[useful_fields].copy()                              \n",
        "  df_X.replace(cleanup_nums, inplace=True)  # convert continous categorial variables to numerical\n",
        "  df_Y = df[target_field].copy()\n",
        "\n",
        "  x = df_X.to_numpy().astype(np.float32)\n",
        "  y = df_Y.to_numpy().astype(np.float32)\n",
        "\n",
        "  if price_max>0:\n",
        "    idxs = y<price_max\n",
        "    x = x[idxs]\n",
        "    y = y[idxs]\n",
        "\n",
        "  if area_max>0:\n",
        "    idxs = x[:,0]<area_max\n",
        "    x = x[idxs]\n",
        "    y = y[idxs]\n",
        "\n",
        "  return (x, y, df) if return_df else (x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jcX7oYDFwzQ"
      },
      "outputs": [],
      "source": [
        "def house_prices_dataset_normed():\n",
        "    x, y = house_prices_dataset(return_df=False, price_max=-1, area_max=-1)\n",
        "    \n",
        "    scaler=StandardScaler()\n",
        "    features_scaled=scaler.fit_transform(x)\n",
        "    \n",
        "    return features_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqWU0eHts1RM"
      },
      "outputs": [],
      "source": [
        "x, y, df = house_prices_dataset(return_df=True)\n",
        "print(x.shape, y.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91nj7znzMEpA"
      },
      "outputs": [],
      "source": [
        "plt.plot(x[:, 0], y, '.')\n",
        "plt.xlabel('area, sq.ft')\n",
        "plt.ylabel('price, $');\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7CNxkPdNB4L"
      },
      "source": [
        "## 3. Blobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8wXhleONKgZ"
      },
      "outputs": [],
      "source": [
        "x, y = make_blobs(n_samples=1000, centers=[[0,0], [5,5], [10, 0]])\n",
        "colors = \"ygr\"\n",
        "for i, color in enumerate(colors):\n",
        "    idx = y == i\n",
        "    plt.scatter(x[idx, 0], x[idx, 1], c=color, edgecolor='gray', s=25)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S1jwU4cXQX4"
      },
      "source": [
        "## 4. MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2u82UQ5XQX4"
      },
      "source": [
        "The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image.\n",
        "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting (taken from http://yann.lecun.com/exdb/mnist/). Each example is a 28x28 grayscale image and the dataset can be readily downloaded from Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaNaGGOkXQX5"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlUY5gl8XQX7"
      },
      "source": [
        "Let's check few samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtYtGEDdXQX8"
      },
      "outputs": [],
      "source": [
        "n = 3\n",
        "fig, ax = plt.subplots(n, n, figsize=(2*n, 2*n))\n",
        "ax = [ax_xy for ax_y in ax for ax_xy in ax_y]\n",
        "for axi, im_idx in zip(ax, np.random.choice(len(train_images), n**2)):\n",
        "  im = train_images[im_idx]\n",
        "  im_class = train_labels[im_idx]\n",
        "  axi.imshow(im, cmap='gray')\n",
        "  axi.text(1, 4, f'{im_class}', color='r', size=16)\n",
        "  axi.grid(False)\n",
        "plt.tight_layout(0,0,0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MXcNBOBCUcy"
      },
      "source": [
        "## 5. Fashion MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rANzstLwCUcy"
      },
      "source": [
        "`Fashion-MNIST` is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. (from https://github.com/zalandoresearch/fashion-mnist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Woc5ld-HCUcz"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH2G-FAHCUc1"
      },
      "source": [
        "Let's check few samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90EhR8nmCUc1"
      },
      "outputs": [],
      "source": [
        "n = 3\n",
        "fig, ax = plt.subplots(n, n, figsize=(2*n, 2*n))\n",
        "ax = [ax_xy for ax_y in ax for ax_xy in ax_y]\n",
        "for axi, im_idx in zip(ax, np.random.choice(len(train_images), n**2)):\n",
        "  im = train_images[im_idx]\n",
        "  im_class = train_labels[im_idx]\n",
        "  axi.imshow(im, cmap='gray')\n",
        "  axi.text(1, 4, f'{im_class}', color='r', size=16)\n",
        "  axi.grid(False)\n",
        "plt.tight_layout(0,0,0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeaJnq9hCUc3"
      },
      "source": [
        "Each of the training and test examples is assigned to one of the following labels:\n",
        "\n",
        "| Label | Description |\n",
        "| --- | --- |\n",
        "| 0 | T-shirt/top |\n",
        "| 1 | Trouser |\n",
        "| 2 | Pullover |\n",
        "| 3 | Dress |\n",
        "| 4 | Coat |\n",
        "| 5 | Sandal |\n",
        "| 6 | Shirt |\n",
        "| 7 | Sneaker |\n",
        "| 8 | Bag |\n",
        "| 9 | Ankle boot |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_wxOrdWko8W"
      },
      "source": [
        "In this course we will use several synthetic and real-world datasets to illustrate the behavior of the models and exercise our skills."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Weather dataset"
      ],
      "metadata": {
        "id": "x2NWxK0BFwyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a weather time series [dataset](https://www.bgc-jena.mpg.de/wetter/) recorded by the Max Planck Institute for Biogeochemistry \n",
        "It contains weather reacord for 8 years of observation."
      ],
      "metadata": {
        "id": "LKsTkrMkGB7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weather_df():\n",
        "  # inspired by https://www.tensorflow.org/tutorials/structured_data/time_series\n",
        "\n",
        "  # download and extract dataset\n",
        "  zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "  csv_path, _ = os.path.splitext(zip_path)\n",
        "\n",
        "  # load into pandas df\n",
        "  df = pd.read_csv(csv_path)\n",
        "  \n",
        "  # dataset contains records every 10 min, we use hourly records only, thus\n",
        "  # slice [start:stop:step], starting from index 5 take every 6th record\n",
        "  df = df[5::6]\n",
        "\n",
        "  # replace errors in wind velocity to 0\n",
        "  wv = df['wv (m/s)']\n",
        "  bad_wv = wv == -9999.0\n",
        "  wv[bad_wv] = 0.0\n",
        "\n",
        "  max_wv = df['max. wv (m/s)']\n",
        "  bad_max_wv = max_wv == -9999.0\n",
        "  max_wv[bad_max_wv] = 0.0\n",
        "\n",
        "  # obtain timestamps from text time format\n",
        "  date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
        "  timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
        "  # genarate cyclic features for year and day\n",
        "  day = 24*60*60\n",
        "  year = (365.2425) * day\n",
        "\n",
        "  df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
        "  df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
        "  df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
        "  df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "D_bXLpwxGTbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df = get_weather_df()"
      ],
      "metadata": {
        "id": "DsYDOZZKI7_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df.head()"
      ],
      "metadata": {
        "id": "yQHfhrBMJA5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df.describe().T"
      ],
      "metadata": {
        "id": "wPUvJcY3JjJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(weather_df['Year cos'])\n",
        "plt.plot(weather_df['Year sin'])"
      ],
      "metadata": {
        "id": "T7qIhgZuMGW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(weather_df['Day cos'][:7*24])\n",
        "plt.plot(weather_df['Day sin'][:7*24])"
      ],
      "metadata": {
        "id": "WxziDnFwLqL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_future_T_dataset(X_len=24, Y_offset=48,\n",
        "                         X_features=['p (mbar)', 'T (degC)', 'rh (%)', 'wv (m/s)', 'wd (deg)', 'Day sin', 'Day cos', 'Year sin', 'Year cos'],\n",
        "                         Y_features='T (degC)',\n",
        "                         standardize=True,\n",
        "                         oversample=10\n",
        "                         ):\n",
        "  \"\"\"\n",
        "  Generates pairs of input-label, using sequence of `X_len` samples as input\n",
        "  and value at offset `Y_offset` from start of this sequence as label.\n",
        "  Sample sequnces arte taken at random positions throughout the dataset.\n",
        "  Number of samples is obtained assuming non-overlaping wondows.\n",
        "  Oversampling factor allows to increase this value.\n",
        "\n",
        "  Args:\n",
        "    X_len (int): length of sample sequence\n",
        "    Y_offset (int): offset to the target value from the sequence start\n",
        "    X_features (list or str): features to be used as input\n",
        "    Y_features (list or str): features to be used as labels\n",
        "    standardize (Bool): flag whether to standardize the columns\n",
        "    oversample (int): increases number of samples by this factor wrt baseline\n",
        "                      n = len(df) // (Y_offset+1)\n",
        "  \"\"\"\n",
        "  weather_df = get_weather_df()\n",
        "\n",
        "  if standardize:\n",
        "    mean = weather_df.mean()\n",
        "    std = weather_df.std()\n",
        "    weather_df = (weather_df - mean) / std\n",
        "  \n",
        "  df_X = weather_df[X_features]\n",
        "  df_Y = weather_df[Y_features]\n",
        "\n",
        "  n_records = len(df_X)\n",
        "  sample_len = Y_offset+1\n",
        "  n_samples = int((n_records-sample_len)//sample_len*oversample)\n",
        "  offsets = np.random.randint(0, n_records-sample_len, size=n_samples)\n",
        "  offsets.sort()\n",
        "\n",
        "  X = []\n",
        "  Y = []\n",
        "  for o in offsets:\n",
        "    X.append(np.array(df_X[o:o+X_len]))\n",
        "    Y.append(np.array(df_Y[o+Y_offset:o+Y_offset+1]))\n",
        "\n",
        "  X = np.stack(X)\n",
        "  Y = np.concatenate(Y)\n",
        "\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "tDQkIP4cJxuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = gen_future_T_dataset()"
      ],
      "metadata": {
        "id": "m-uVf1AOPLIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape, y.shape"
      ],
      "metadata": {
        "id": "m2f1pWLfPbJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for f,fn in enumerate(['p (mbar)', 'T (degC)', 'rh (%)', 'wv (m/s)', 'wd (deg)', 'Day sin', 'Day cos', 'Year sin', 'Year cos']):\n",
        "  plt.figure(figsize=(5*(1+(f==1)), 4))\n",
        "  for s in range(10):\n",
        "    plt.plot(x[s, :, f])\n",
        "    if f==1:\n",
        "      plt.scatter(48, y[s], color=plt.gca().lines[-1].get_color())\n",
        "  plt.title(fn)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "JJU5MRbdRDEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9ARiWSS91kO"
      },
      "source": [
        "# 1. Unsupervised Learning Techniques\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afROKfT591kS"
      },
      "source": [
        "## 1. Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXqNWQWT91kU"
      },
      "source": [
        "### Theory overview."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVw0KP4_91kW"
      },
      "source": [
        "**Objective:** PCA is used for dimensionality reduction when we have a large number $D$ of features with non-trivial intercorrelation ( data redundancy ) and to isolate relevant features.\n",
        "\n",
        "PCA provides a new set of $M$ uncorrelated features for every data point, with $M \\le D$. The new features are:\n",
        "\n",
        "- a linear combination of the original ones ; \n",
        "- uncorrelated between each other ; \n",
        "\n",
        "If $M \\ll D$ we get an effective dimensionality reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnnFBIaf91kX"
      },
      "source": [
        "**Methods:** Each data point indexed by $p=1..N$ can be seen as an element $\\mathbf{x}_p \\in \\mathbf{R}^D$. \n",
        "\n",
        "The variance of the data-cloud measures the spread around its centroid:\n",
        "\n",
        "$$S^2=\\frac{1}{N}\\sum_{p=1}^{N}  ( \\mathbf{x}_p - \\mathbf{\\overline{x}})^2$$\n",
        "$$\\mathbf{\\overline{x}}=\\frac{1}{N}\\sum_{p=1}^{N} \\mathbf{x}_p$$\n",
        "\n",
        "We fix a number $1\\le k \\le D$ and consider a subspace $V_k$ of dimension $k$. Each data point  $\\mathbf{x}_p$ is projected onto $V_k$, leading to points $\\mathbf{x}^k_p$ with spread $S^{2,k}$.  PCA chooses $V_k$ such that the variance $S^{2,k}$ is maximized, as shown in the picture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwbKF5LZijSQ"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/pca-theory.png\" width=\"100%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ySGaLWp91kZ"
      },
      "source": [
        "**Terminology and output of a PCA computation:** \n",
        "- `Principal components`: A sequence of orthonormal vectors $k_1,..,k_n$ spanning optimal subspaces: $\\text{Span}\\{k_1,..,k_m\\}=V_m$ ; \n",
        "- `Scores`: For every sample-point $p$. the new features are called scores are given by the component of $p$ along the $k$ vectors;  \n",
        "- `Reconstructed vector`: For every $k$, the projection of $V$ on $V_k$ ;\n",
        "- `Explained variance`: For every k, the ratio between the variance of the reconstructed vectors and total variance. The number of components is chosen selecting an optimal k. The plot of the explained variance as a function of k is called a *scree plot*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF-FTX_P91ka"
      },
      "source": [
        "### Sklearn: implementation and usage of PCA.\n",
        "\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGwCp73k91kb"
      },
      "source": [
        "We start showing a two-dimensional example that can be easy visualized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoEWiYnL91ko"
      },
      "source": [
        "We load the datasets that we are going to use for the examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ4SGUVv91kq"
      },
      "outputs": [],
      "source": [
        "data=load_sample_data_pca()\n",
        "\n",
        "n_samples,n_dim=data.shape\n",
        "\n",
        "print('We have ',n_samples, 'samples of dimension ', n_dim)\n",
        "\n",
        "plt.figure(figsize=((5,5)))\n",
        "plt.grid()\n",
        "plt.plot(data[:,0],data[:,1],'o')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzvgzzfA91k8"
      },
      "source": [
        "The data set is almost one dimensional. PCA will confirm this result.\n",
        "\n",
        "As with most of sklearn functionalities, we need first to create a PCA object. We will use the object methods to perform PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6poli6o91k_"
      },
      "outputs": [],
      "source": [
        "pca=PCA(n_components=2) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju0UCpkP91lK"
      },
      "source": [
        "A call to the pca.fit method computes the principal components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifWK32ME91lM"
      },
      "outputs": [],
      "source": [
        "pca.fit(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZdfZbLv91lW"
      },
      "source": [
        "Now the pca.components_ attribute contains the principal components. We can print them alongside with the data and check that they constitute an orthonormal basis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-RnE3H491lY"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=((5,5)))\n",
        "plt.grid()\n",
        "plt.plot(data[:,0],data[:,1],'o')\n",
        "\n",
        "circle=plt.Circle((0, 0), 1.0, linestyle='--', color='red',fill=False)\n",
        "ax=plt.gca()\n",
        "ax.add_artist(circle)\n",
        "\n",
        "for vec in pca.components_:\n",
        "    plt.quiver([0], [0], [vec[0]], [vec[1]], angles='xy', scale_units='xy', scale=1)\n",
        "\n",
        "plt.xlim(-2,2)\n",
        "plt.ylim(-2,2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q48o3r3t91li"
      },
      "source": [
        "The pca.explained_variance_ratio_ attribute contains the explained variance. In this case we see that already the first reconstructed vector explains 95% of the variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwEopdTN91lk"
      },
      "outputs": [],
      "source": [
        "print(pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfaA1wo391ls"
      },
      "source": [
        "To compute the reconstructed vectors for k=1 we first need to compute the scores and then multiply by the basis vectors:\n",
        "\n",
        "$\\mathbf x_{rec}=\\sum_i (\\mathbf x \\cdot \\mathbf v^{pr}_i) \\mathbf v^{pr}_i$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enRkHTyO91lu"
      },
      "outputs": [],
      "source": [
        "k=1\n",
        "scores=pca.transform(data)\n",
        "res=np.dot(scores[:,:k], pca.components_[:k,:] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpoIobhk91ly"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=((5,5)))\n",
        "plt.plot(res[:,0],res[:,1],'o')\n",
        "plt.plot(data[:,0],data[:,1],'o')\n",
        "\n",
        "for a,b,c,d in zip(data[:,0],data[:,1],res[:,0],res[:,1]) :\n",
        "    plt.plot([a,c],[b,d],'-', linestyle = '--', color='red')\n",
        "\n",
        "plt.grid()\n",
        "\n",
        "plt.xlim(-1.0,1.0)\n",
        "plt.ylim(-1.0,1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZlmDioD91mD"
      },
      "source": [
        "The same procedure is followed for high dimensional datasets. Here we generate random data which lies almost on a 6-dimensional subspace. The resulting scree plot can be used to find this result in a semi-automatic fashion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SghqE-fW91mL"
      },
      "outputs": [],
      "source": [
        "high_dim_dataset=load_multidimensional_data_pca(n_data=40 ,n_vec=6, dim=20, eps= 0.5)\n",
        "n_samples,n_dim=high_dim_dataset.shape\n",
        "\n",
        "print('We have ',n_samples, 'samples of dimension ', n_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4m8mWyS91mT"
      },
      "outputs": [],
      "source": [
        "pca=PCA()\n",
        "pca.fit(high_dim_dataset)\n",
        "plt.plot(pca.explained_variance_ratio_,'-o')\n",
        "plt.title('Scree plot')\n",
        "plt.ylabel('Percentage of explained variance')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjXiNHd-91mb"
      },
      "source": [
        "As an exercise, you can change the value of eps and see how the screen plot changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nJsEk2R91nM"
      },
      "source": [
        "### EXERCISE 1 : Find the hidden drawing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QiWejkZt9ne"
      },
      "outputs": [],
      "source": [
        "### In this exercise you will take a high dimensional dataset, find the optimal number of principal components \n",
        "# and visualize the reconstructed vectors with k=2. The pipeline is the same as Ex. 2.\n",
        "\n",
        "# 1. Load the data using the function data=load_ex2_data_pca(seed=1235) , check the dimensionality of the data and plot them.\n",
        "data=...\n",
        "n_samples,n_dim=... \n",
        "print('We have ',n_samples, 'samples of dimension ', n_dim)\n",
        "\n",
        "# 2. Define a PCA object and perform the PCA fitting.\n",
        "pca=PCA()\n",
        "pca....(data)\n",
        "\n",
        "# 3. Check the explained variance ratio and select best number of components.\n",
        "print(pca...)\n",
        "plt.plot(...)\n",
        "\n",
        "# 4. Plot the reconstructed vectors for the best value of data=...\n",
        "k=...\n",
        "data_transformed=...\n",
        "plt.plot(data_transformed[:,...],data_transformed[:,...],'o')\n",
        "plt.xlabel('First component')\n",
        "plt.ylabel('Second component')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3AtpLmQef_R"
      },
      "source": [
        "### Final comments:\n",
        "\n",
        "PCA is able therefore to make this mapping:\n",
        "\n",
        "$(x_1,...,x_D) \\rightarrow (y_1,..,y_M)$\n",
        "\n",
        "Here we focused on data compression, but it is also very important that $y_1,...,y_M$ are uncorrolated for interpratibility purposes. Being uncorrelated means (roughly) that in our dataset we can change one variable without affecting the others. The dimensions 1,...,M are often therefore more interpretable and providing more information.\n",
        "\n",
        "See e.g. a similar application here:\n",
        "\n",
        "\"Principal component analysis of dietary and lifestyle patterns in relation to risk of subtypes of esophageal and gastric cancer\", https://pubmed.ncbi.nlm.nih.gov/21435900/\n",
        "\n",
        ", where each data point $x$ is an answer from a questionnaire of food. The principal components are than typical \"patterns\" of answers that are uncorrlated, have a look at table 2, and if you want read the whole data story :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEK5-Ksg91nw"
      },
      "source": [
        "## 2. Data visualization and embedding in low dimensions ( t-SNE / UMAP )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rP48tTL91nz"
      },
      "source": [
        "### Theory overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Jer5L-91n0"
      },
      "source": [
        "PCA is a linear embedding technique where the scores are a linear function of the original variables. This forces the number of principal components to be used to be high, if the manifold is highly non-linear. Curved manifolds need to be embedded in higher dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX4zvhaN91n2"
      },
      "source": [
        "Other `non-linear` embedding techniques start from a local description of the environment of each sample point in the original space:\n",
        "\n",
        "- `t-Sne` uses a `statistical description` of the environment of a sample point ;\n",
        "- `UMAP` describes the `topology` of the environment through a generalized \"triangulation\" (simplex decomposition) ;\n",
        "\n",
        "The projection on the low-dimensional space is optimized in order to match as much as possible the description of the local environment. \n",
        "\n",
        "It is not the goal of this introduction to discuss the derivation of such approaches, which can be found in the references:\n",
        "\n",
        "https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf\n",
        "\n",
        "https://arxiv.org/pdf/1802.03426.pdf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhP5EIc791n7"
      },
      "source": [
        "Instead, the following, we will show how to apply practically these dimensionality reductions techniques. Keep in mind that the embedding is given by an iterative solution of a minimization problem and therefore the results may depend on the value of the random seed, especially for t-SNE visualizazions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwIggNFN91n8"
      },
      "source": [
        "### Utilization in Python and examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP5RbzRa91n9"
      },
      "source": [
        "To begin with, we create a t-SNE object that we are going to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMZtUAyR91n9"
      },
      "outputs": [],
      "source": [
        "tsne_model = TSNE(perplexity=30, n_components=2, learning_rate=200, early_exaggeration=4.0,init='pca', \n",
        "                      n_iter=2000, random_state=2233212, metric='euclidean', verbose=100 )\n",
        "\n",
        "umap_model = umap.UMAP(n_neighbors=30, n_components=2, random_state=1711)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-iq56Yn91oD"
      },
      "source": [
        "### Example 1: Exercise 3 Cont'd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFZ_RAwj91oE"
      },
      "source": [
        "We will first visualize our multi-dimensional heart using t-SNE: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TFExSbC91oF",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "data= load_ex2_data_pca(seed=1235, n_add=20)\n",
        "\n",
        "tsne_model = TSNE(perplexity=30, n_components=2, learning_rate=200, early_exaggeration=4.0,init='pca', \n",
        "                      n_iter=300, random_state=2233212, metric='euclidean', verbose=100 )\n",
        "\n",
        "tsne_heart = tsne_model.fit_transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fl8_wlp91oJ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.scatter(tsne_heart[:,0],tsne_heart[:,1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0R8p_0E91oR"
      },
      "source": [
        "And using UMAP :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUU2YauU91oT"
      },
      "outputs": [],
      "source": [
        "umap_model = umap.UMAP(n_neighbors=30, n_components=2, random_state=1711)\n",
        "\n",
        "umap_hart = umap_model.fit_transform(data)\n",
        "plt.scatter(umap_hart[:, 0], umap_hart[:, 1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kot3Prcv91oi"
      },
      "source": [
        "### Example 2: Mnist dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZskLulw91oj"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "n_examples = 5000\n",
        "data=train_images[:n_examples,:].reshape(n_examples,-1)\n",
        "data=data/255\n",
        "\n",
        "labels=train_labels[:n_examples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDeL_V2a91op",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# not to run on COLAB\n",
        "\n",
        "# tsne_model = TSNE(perplexity=10, n_components=2, learning_rate=200,\n",
        "#                   early_exaggeration=4.0,init='pca', \n",
        "#                   n_iter=2000, random_state=2233212, \n",
        "#                   metric='euclidean', verbose=100, n_jobs=1)\n",
        "\n",
        "# tsne_mnist = tsne_model.fit_transform(data)\n",
        "\n",
        "# plt.scatter(tsne_mnist[:,0],tsne_mnist[:,1],c=labels,s=10)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRwwHbqpt9n0"
      },
      "source": [
        " | <img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/t_sne_mnist.png\" width=\"100%\"/> | <img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/mnist.png\" width=\"100%\"/> |\n",
        " |  -----:| -----:|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C4HVS9B91o4"
      },
      "outputs": [],
      "source": [
        "umap_model = umap.UMAP(n_neighbors=10, n_components=2, random_state=1711)\n",
        "umap_mnist = umap_model.fit_transform(data)\n",
        "plt.scatter(umap_mnist[:, 0], umap_mnist[:, 1], c=labels, s=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPOM-hCV91pB"
      },
      "source": [
        "### Example 3: Fashion_Mnist dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq3DPRiV91pF"
      },
      "outputs": [],
      "source": [
        "fmnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fmnist.load_data()\n",
        "\n",
        "n_examples = 5000\n",
        "data=train_images[:n_examples,:].reshape(n_examples,-1)\n",
        "data=data/255\n",
        "\n",
        "labels=train_labels[:n_examples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytrl7jyC91pR",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# not to run on COLAB\n",
        "\n",
        "# tsne_model = TSNE(perplexity=50, n_components=2, learning_rate=200, early_exaggeration=4.0,init='pca', \n",
        "#                      n_iter=1000, random_state=2233212, metric='euclidean', verbose=100 )\n",
        "\n",
        "# tsne_fmnist = tsne_model.fit_transform(data)\n",
        "\n",
        "# plt.scatter(tsne_fmnist[:,0],tsne_fmnist[:,1],c=labels,s=10)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUnsyxnet9n9"
      },
      "source": [
        " | <img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/t_sne_fmnist.png\" width=\"100%\"/> | <img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/fmnist.png\" width=\"100%\"/> |\n",
        " |  -----:| -----:|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xq3vZFJy91p8"
      },
      "outputs": [],
      "source": [
        "umap_model = umap.UMAP(n_neighbors=50, n_components=2, random_state=1711)\n",
        "umap_fmnist = umap_model.fit_transform(data)\n",
        "plt.scatter(umap_fmnist[:, 0], umap_fmnist[:, 1], c=labels, s=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz2LEGDP91qM"
      },
      "source": [
        "### Example 4: House prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eycvatmq91qO"
      },
      "outputs": [],
      "source": [
        "data=house_prices_dataset_normed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82secxju91qZ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# not to run on COLAB\n",
        "\n",
        "#tsne_model = TSNE(perplexity=30, n_components=2, learning_rate=200,\n",
        "#                  early_exaggeration=4.0,init='pca', n_iter=1000,\n",
        "#                  random_state=2233212, metric='euclidean', verbose=100)\n",
        "\n",
        "#tsne_houses = tsne_model.fit_transform(data)\n",
        "\n",
        "#plt.scatter(tsne_houses[:,0],tsne_houses[:,1],s=20)\n",
        "#plt.savefig('t_sne_houses.png')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESjUplVMt9oF"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/t_sne_houses.png\" width=\"50%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWeCP6pN91qr"
      },
      "outputs": [],
      "source": [
        "umap_model = umap.UMAP(n_neighbors=30, n_components=2, random_state=1711)\n",
        "umap_houses = umap_model.fit_transform(data)\n",
        "plt.scatter(umap_houses[:, 0], umap_houses[:, 1], s=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B3wGs2Q91rE"
      },
      "source": [
        "**Message:** Visualization techniques are useful for having an initial grasp of multi-dimensional datasets and guide further analysis and the choice of the modelling data strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78xdYF9-B-jA"
      },
      "source": [
        "**Caveats:** \n",
        "- available t-SNE implementations may vary a lot in terms of performance. Computational time can be reduced performing PCA before a t-SNE projection\n",
        "\n",
        "- UMAP, thanks to the algorithm being amanable to clever initializations and optimization schemes, offers great stability and scaling properties\n",
        "\n",
        "- UMAP, even if starting from a local picture, is generally more able to spread apart different clusters\n",
        "\n",
        "- The result of an embedding may depend on the values of the metaparameters. One should try to see how the final embedding changes in order to get to a complete picture"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ITfbaOgfYNsq",
        "7-CGSS2OZKHD",
        "Bxtv48o-F1Ku",
        "l582Sr0_WGXj",
        "EHZ-hHGuY5aG",
        "puQNgKN0wS7H",
        "vhhycm2S6wbz",
        "afROKfT591kS",
        "1B7g_pGO91md"
      ],
      "name": "Course_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}